{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b62584-faf9-4129-a30d-02bd6f3791d8",
   "metadata": {},
   "source": [
    "# Hadoop & Apache Spark Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2639cc4-193b-4927-b450-869951a9f95e",
   "metadata": {},
   "source": [
    "In the below notebook I chose to leverage both Hadoop and Apache Spark big data solutions for data storage and pre-processing purposes.\n",
    "\n",
    "By using these tools in collaboration with each other I chose to combine the strengths of both platforms, offering a powerful solution for processing big data.\n",
    "\n",
    "Hadoop was chosen for it's efficient data storage via the Hadoop Distributed File System (HDFS), which provides a reliable and scalable storage solution that's designed to handle very large datasets.\n",
    "\n",
    "Whereas Apache Spark was chosen for it's data processing capabilities allowing me to complete the necessary data pre-processing required prior to further analysis and modeling. \n",
    "\n",
    "PySpark is a much more user friendly and easy to use Python API in comparison to MapReduce which is difficult to use given it's complex syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4144c7-e33c-48d1-80e7-26fbe647f348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return SparkContext, connecting Spark Cluster\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea489df-79a1-488f-ba76-929c180495fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc master - running locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b82e28-dc95-43fd-b0e4-47a7cd861516",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "The above steps were taken to allow me to connect to Apache Spark via SparkContext by initialising my Spark application which sets up the necessary underlying Spark functionality.\n",
    "\n",
    "By confirming that Spark is running locally it means that I am utilising all available cores on my machine, which should result in the execution of my Spark application being as fast as possible given my current hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23cc7e51-6dfc-4ca2-bad6-24032f48106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from pyspark.sql.functions import col, count, when, to_date, year, month, dayofmonth, row_number, avg, min, max, mean, sum, round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8543426d-d601-4133-b5f8-0d6679ff5595",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "The above packages were imported to allow for efficient data analysis, manipulation and pre-processing using PySpark's capabilities. Given that I'm using PySpark I cannot rely on commonly used Python libraries for EDA such as Pandas or Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9f2d2cf-6b22-4be2-a435-1a50ecae9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file directly from my Hadoop directory\n",
    "df = spark.read.csv('/user1/GSK_stock_data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0bdea-3eb7-419d-98aa-7ac8a0db4c52",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "Having first previously copied my dataset into my local Hadoop directory and ensured that Hadoop is successfully running on my local environment I then can read it into this Jupyter notebook using PySpark which ensures that I am leveraging both the storage capabilities of Hadoop and processing capabilites of Apache Spark efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3481a0-af55-4af0-bdaa-4851e9bdb3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Dividends: double (nullable = true)\n",
      " |-- Stock Splits: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display df structure\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0c9dae2-c81b-4818-90d8-8443f212d80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-------------------+-------------------+------+---------+------------+\n",
      "|               Date|               Open|               High|                Low|              Close|Volume|Dividends|Stock Splits|\n",
      "+-------------------+-------------------+-------------------+-------------------+-------------------+------+---------+------------+\n",
      "|1980-03-28 06:00:00|                0.0|0.14964139573276042| 0.1392012983560562| 0.1392012983560562|  2400|      0.0|         0.0|\n",
      "|1980-03-31 06:00:00| 0.1392012983560562| 0.1392012983560562| 0.1392012983560562| 0.1392012983560562|     0|      0.0|         0.0|\n",
      "|1980-04-01 06:00:00| 0.1392012983560562| 0.1392012983560562| 0.1392012983560562| 0.1392012983560562|     0|      0.0|         0.0|\n",
      "|1980-04-02 06:00:00|                0.0|0.14964139573276042| 0.1392012983560562| 0.1392012983560562|   800|      0.0|         0.0|\n",
      "|1980-04-03 06:00:00| 0.1392012983560562| 0.1392012983560562| 0.1392012983560562| 0.1392012983560562|     0|      0.0|         0.0|\n",
      "|1980-04-07 06:00:00|                0.0|0.14964139573276042| 0.1392012983560562| 0.1392012983560562|  2400|      0.0|         0.0|\n",
      "|1980-04-08 06:00:00| 0.1392012983560562| 0.1392012983560562| 0.1392012983560562| 0.1392012983560562|     0|      0.0|         0.0|\n",
      "|1980-04-09 06:00:00| 0.1392012983560562| 0.1392012983560562| 0.1392012983560562| 0.1392012983560562|     0|      0.0|         0.0|\n",
      "|1980-04-10 06:00:00| 0.1392012983560562| 0.1392012983560562| 0.1392012983560562| 0.1392012983560562|     0|      0.0|         0.0|\n",
      "|1980-04-11 06:00:00|                0.0|0.15312145977485472|0.14268136024475098|0.14268136024475098|  1600|      0.0|         0.0|\n",
      "|1980-04-14 06:00:00|0.14268136024475098|0.14268136024475098|0.14268136024475098|0.14268136024475098|     0|      0.0|         0.0|\n",
      "|1980-04-15 06:00:00|                0.0|0.14268133161883606|0.13224123418331146|0.13224123418331146|  4000|      0.0|         0.0|\n",
      "|1980-04-16 06:00:00|                0.0|0.14268133161883606|0.13224123418331146|0.13224123418331146|  4000|      0.0|         0.0|\n",
      "|1980-04-17 06:00:00|0.13224123418331146|0.13224123418331146|0.13224123418331146|0.13224123418331146|     0|      0.0|         0.0|\n",
      "|1980-04-18 06:00:00|0.13224123418331146|0.13224123418331146|0.13224123418331146|0.13224123418331146|     0|      0.0|         0.0|\n",
      "|1980-04-21 06:00:00|0.13224123418331146|0.13224123418331146|0.13224123418331146|0.13224123418331146|     0|      0.0|         0.0|\n",
      "|1980-04-22 06:00:00|0.13224123418331146|0.13224123418331146|0.13224123418331146|0.13224123418331146|     0|      0.0|         0.0|\n",
      "|1980-04-23 06:00:00|0.13224123418331146|0.13224123418331146|0.13224123418331146|0.13224123418331146|     0|      0.0|         0.0|\n",
      "|1980-04-24 06:00:00|0.13224123418331146|0.13224123418331146|0.13224123418331146|0.13224123418331146|     0|      0.0|         0.0|\n",
      "|1980-04-25 06:00:00|0.13224123418331146|0.13224123418331146|0.13224123418331146|0.13224123418331146|     0|      0.0|         0.0|\n",
      "+-------------------+-------------------+-------------------+-------------------+-------------------+------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a5aac3b-6cfd-4586-b355-04067ba83550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/11 19:13:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "|summary|              Open|               High|                Low|              Close|            Volume|           Dividends|        Stock Splits|\n",
      "+-------+------------------+-------------------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "|  count|             11079|              11079|              11079|              11079|             11079|               11079|               11079|\n",
      "|   mean| 15.99001955803962| 16.187680846803783| 15.962643930768905| 16.077199139687515|1938348.9213827963|0.005214775701778139|7.244336131419804E-4|\n",
      "| stddev|11.896278348218647| 11.851240420274959| 11.717892849249225| 11.785981846870502|1832170.7216027784| 0.05279006919486163|0.035723332697144526|\n",
      "|    min|               0.0|0.11484106630086899|0.11484106630086899|0.11484106630086899|                 0|                 0.0|                 0.0|\n",
      "|    max| 43.68000030517578|  43.84000015258789|  43.47999954223633|  43.58000183105469|          35389900|               1.252|                 2.0|\n",
      "+-------+------------------+-------------------+-------------------+-------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the data\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d09ab958-bc60-4c0d-a800-348feb7ed2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines in file: 11079\n"
     ]
    }
   ],
   "source": [
    "# Count lines\n",
    "print('number of lines in file: %s' % df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0da70-5c4c-418b-a23a-6b634aad8490",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "The above Exploratory Data Analysis (EDA) steps were taken to ensure that my data has loaded successfully into a Spark DataFrame as well as confirming the datatypes, number of records and general descriptive statistics. \n",
    "\n",
    "Apache Spark packages were imported to allow for efficient data analysis, manipulation and pre-processing using PySpark's capabilities. Given that I'm using PySpark I cannot rely on commonly used Python libraries for EDA such as Pandas or Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "222ca368-106a-4d3a-ac17-6acc8dbde6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+------+---------+------------+\n",
      "|Date|Open|High|Low|Close|Volume|Dividends|Stock Splits|\n",
      "+----+----+----+---+-----+------+---------+------------+\n",
      "|   0|   0|   0|  0|    0|     0|        0|           0|\n",
      "+----+----+----+---+-----+------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af498af-744a-43cc-a9bf-3add7ad87914",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "The above step were taken to ensure assess whether or not my data has any missing values which thankfully it doesn't meaning I don't need to consider this as part of future modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94bf6378-a2fa-4023-8395-a6c113a3aacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------------------+------------------+------------------+------+---------+------------+\n",
      "|      Date|              Open|               High|               Low|             Close|Volume|Dividends|Stock Splits|\n",
      "+----------+------------------+-------------------+------------------+------------------+------+---------+------------+\n",
      "|1980-03-28|               0.0|0.14964139573276042|0.1392012983560562|0.1392012983560562|  2400|      0.0|         0.0|\n",
      "|1980-03-31|0.1392012983560562| 0.1392012983560562|0.1392012983560562|0.1392012983560562|     0|      0.0|         0.0|\n",
      "|1980-04-01|0.1392012983560562| 0.1392012983560562|0.1392012983560562|0.1392012983560562|     0|      0.0|         0.0|\n",
      "|1980-04-02|               0.0|0.14964139573276042|0.1392012983560562|0.1392012983560562|   800|      0.0|         0.0|\n",
      "|1980-04-03|0.1392012983560562| 0.1392012983560562|0.1392012983560562|0.1392012983560562|     0|      0.0|         0.0|\n",
      "+----------+------------------+-------------------+------------------+------------------+------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Date' column from Datetime to regular Date format\n",
    "df = df.withColumn('Date', to_date(col('Date'), 'yyyy-MM-dd'))\n",
    "\n",
    "df.show(5)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea43468-79e5-4449-a50a-0c7b5fc0db8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------------------+------------------+------------------+------+---------+------------+----+-----+---+\n",
      "|      Date|              Open|               High|               Low|             Close|Volume|Dividends|Stock Splits|Year|Month|Day|\n",
      "+----------+------------------+-------------------+------------------+------------------+------+---------+------------+----+-----+---+\n",
      "|1980-03-28|               0.0|0.14964139573276042|0.1392012983560562|0.1392012983560562|  2400|      0.0|         0.0|1980|    3| 28|\n",
      "|1980-03-31|0.1392012983560562| 0.1392012983560562|0.1392012983560562|0.1392012983560562|     0|      0.0|         0.0|1980|    3| 31|\n",
      "|1980-04-01|0.1392012983560562| 0.1392012983560562|0.1392012983560562|0.1392012983560562|     0|      0.0|         0.0|1980|    4|  1|\n",
      "|1980-04-02|               0.0|0.14964139573276042|0.1392012983560562|0.1392012983560562|   800|      0.0|         0.0|1980|    4|  2|\n",
      "|1980-04-03|0.1392012983560562| 0.1392012983560562|0.1392012983560562|0.1392012983560562|     0|      0.0|         0.0|1980|    4|  3|\n",
      "+----------+------------------+-------------------+------------------+------------------+------+---------+------------+----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract different date compoments for different levels of analysis\n",
    "df = df.withColumn('Year', year(col('Date')))\\\n",
    "    .withColumn('Month', month(col('Date')))\\\n",
    "    .withColumn('Day', dayofmonth(col('Date')))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d918b43-67af-40bb-9ba4-5f50d5dcc379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------------------+------------------+------------------+------+---------+------------+----+-----+---+------------------+\n",
      "|      Date|              Open|               High|               Low|             Close|Volume|Dividends|Stock Splits|Year|Month|Day|       DailyChange|\n",
      "+----------+------------------+-------------------+------------------+------------------+------+---------+------------+----+-----+---+------------------+\n",
      "|1980-03-28|               0.0|0.14964139573276042|0.1392012983560562|0.1392012983560562|  2400|      0.0|         0.0|1980|    3| 28|0.1392012983560562|\n",
      "|1980-03-31|0.1392012983560562| 0.1392012983560562|0.1392012983560562|0.1392012983560562|     0|      0.0|         0.0|1980|    3| 31|               0.0|\n",
      "|1980-04-01|0.1392012983560562| 0.1392012983560562|0.1392012983560562|0.1392012983560562|     0|      0.0|         0.0|1980|    4|  1|               0.0|\n",
      "|1980-04-02|               0.0|0.14964139573276042|0.1392012983560562|0.1392012983560562|   800|      0.0|         0.0|1980|    4|  2|0.1392012983560562|\n",
      "|1980-04-03|0.1392012983560562| 0.1392012983560562|0.1392012983560562|0.1392012983560562|     0|      0.0|         0.0|1980|    4|  3|               0.0|\n",
      "+----------+------------------+-------------------+------------------+------------------+------+---------+------------+----+-----+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new feature to calculate the daily change in price\n",
    "df = df.withColumn('DailyChange', col('Close') - col('Open'))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efcafd45-70f1-4cb2-85d6-4710042a609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------------------+------------------+------------------+------+---------+------------+----+-----+---+------------------+----------+\n",
      "|      Date|              Open|               High|               Low|             Close|Volume|Dividends|Stock Splits|Year|Month|Day|       DailyChange|PriceTrend|\n",
      "+----------+------------------+-------------------+------------------+------------------+------+---------+------------+----+-----+---+------------------+----------+\n",
      "|1980-03-28|               0.0|0.14964139573276042|0.1392012983560562|0.1392012983560562|  2400|      0.0|         0.0|1980|    3| 28|0.1392012983560562|        Up|\n",
      "|1980-03-31|0.1392012983560562| 0.1392012983560562|0.1392012983560562|0.1392012983560562|     0|      0.0|         0.0|1980|    3| 31|               0.0|      Down|\n",
      "|1980-04-01|0.1392012983560562| 0.1392012983560562|0.1392012983560562|0.1392012983560562|     0|      0.0|         0.0|1980|    4|  1|               0.0|      Down|\n",
      "|1980-04-02|               0.0|0.14964139573276042|0.1392012983560562|0.1392012983560562|   800|      0.0|         0.0|1980|    4|  2|0.1392012983560562|        Up|\n",
      "|1980-04-03|0.1392012983560562| 0.1392012983560562|0.1392012983560562|0.1392012983560562|     0|      0.0|         0.0|1980|    4|  3|               0.0|      Down|\n",
      "+----------+------------------+-------------------+------------------+------------------+------+---------+------------+----+-----+---+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new feature to indicate the price trend\n",
    "df = df.withColumn('PriceTrend', when(col('Close') > col('Open'), 'Up').otherwise('Down'))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "262d8d07-df58-4b75-a2bc-2a52174c0dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Dividends: double (nullable = true)\n",
      " |-- Stock Splits: double (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- DailyChange: double (nullable = true)\n",
      " |-- PriceTrend: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display df structure\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5912018a-fae8-4f8c-91a3-1a2c3013a07b",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "The above steps were taken to ensure my datatypes are in the desirable format for analysis purposes. I decided to create additional features also which will help with my future analysis and modeling, as well as showcasing some of the data transformation functionality of Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "127d09e2-7ca3-403c-8d52-5d76bf9fa3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------------+-------------------+------------------+-----------+\n",
      "|Year|Month|            MaxHigh|             MinLow|          AvgClose|TotalVolume|\n",
      "+----+-----+-------------------+-------------------+------------------+-----------+\n",
      "|1990|    7|  3.857992510157307| 3.4841568830634366|3.6835364954812184|   35596600|\n",
      "|1997|   11|  14.66772187775155| 12.727858049966924|13.767591526633815|    8155100|\n",
      "|2022|   10| 31.555332678789046| 28.057563806574937|29.472206115722656|  125076700|\n",
      "|1980|    8|0.16008152379546053|0.14268136024475098|  0.14715566663515|      42400|\n",
      "|1987|   10|   3.25360755468237| 1.7594612962321232| 2.561120417985049|   65422000|\n",
      "+----+-----+-------------------+-------------------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new aggregated df for exploration\n",
    "df_grouped = df.groupBy('Year', 'Month').agg(max('High').alias('MaxHigh'),\n",
    "                                             min('Low').alias('MinLow'),\n",
    "                                             mean('Close').alias('AvgClose'),\n",
    "                                             sum('Volume').alias('TotalVolume'))\n",
    "\n",
    "df_grouped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38b2971b-6949-4284-a4e4-e54e4b005fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- MaxHigh: double (nullable = true)\n",
      " |-- MinLow: double (nullable = true)\n",
      " |-- AvgClose: double (nullable = true)\n",
      " |-- TotalVolume: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display grouped_df structure\n",
    "df_grouped.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff3162f0-6d27-4a9a-8820-d58c0b2938c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------------+-------------------+------------------+-----------+--------------+-------------+---------------+\n",
      "|Year|Month|            MaxHigh|             MinLow|          AvgClose|TotalVolume|MaxHighRounded|MinLowRounded|AvgCloseRounded|\n",
      "+----+-----+-------------------+-------------------+------------------+-----------+--------------+-------------+---------------+\n",
      "|1990|    7|  3.857992510157307| 3.4841568830634366|3.6835364954812184|   35596600|          3.86|         3.48|           3.68|\n",
      "|1997|   11|  14.66772187775155| 12.727858049966924|13.767591526633815|    8155100|         14.67|        12.73|          13.77|\n",
      "|2022|   10| 31.555332678789046| 28.057563806574937|29.472206115722656|  125076700|         31.56|        28.06|          29.47|\n",
      "|1980|    8|0.16008152379546053|0.14268136024475098|  0.14715566663515|      42400|          0.16|         0.14|           0.15|\n",
      "|1987|   10|   3.25360755468237| 1.7594612962321232| 2.561120417985049|   65422000|          3.25|         1.76|           2.56|\n",
      "+----+-----+-------------------+-------------------+------------------+-----------+--------------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new features rounded to 2 decimal points\n",
    "df_grouped = df_grouped.withColumn('MaxHighRounded', round(col('MaxHigh'), 2))\n",
    "df_grouped = df_grouped.withColumn('MinLowRounded', round(col('MinLow'), 2))\n",
    "df_grouped = df_grouped.withColumn('AvgCloseRounded', round(col('AvgClose'), 2))\n",
    "\n",
    "df_grouped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94f82c0d-aaff-448b-b6fa-1fd62b098ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+--------------+-------------+---------------+\n",
      "|Year|Month|TotalVolume|MaxHighRounded|MinLowRounded|AvgCloseRounded|\n",
      "+----+-----+-----------+--------------+-------------+---------------+\n",
      "|1990|    7|   35596600|          3.86|         3.48|           3.68|\n",
      "|1997|   11|    8155100|         14.67|        12.73|          13.77|\n",
      "|2022|   10|  125076700|         31.56|        28.06|          29.47|\n",
      "|1980|    8|      42400|          0.16|         0.14|           0.15|\n",
      "|1987|   10|   65422000|          3.25|         1.76|           2.56|\n",
      "+----+-----+-----------+--------------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# List columns to drop\n",
    "columns_to_drop = ['MaxHigh', 'MinLow', 'AvgClose']\n",
    "\n",
    "# Drop columns\n",
    "df_grouped = df_grouped.drop(*columns_to_drop)\n",
    "\n",
    "df_grouped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e6eb7c3-f047-40b8-8676-c0491217ea8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+-------+------+--------+\n",
      "|Year|Month|TotalVolume|MaxHigh|MinLow|AvgClose|\n",
      "+----+-----+-----------+-------+------+--------+\n",
      "|1990|    7|   35596600|   3.86|  3.48|    3.68|\n",
      "|1997|   11|    8155100|  14.67| 12.73|   13.77|\n",
      "|2022|   10|  125076700|  31.56| 28.06|   29.47|\n",
      "|1980|    8|      42400|   0.16|  0.14|    0.15|\n",
      "|1987|   10|   65422000|   3.25|  1.76|    2.56|\n",
      "+----+-----+-----------+-------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define new column names without the 'Rounded' suffix\n",
    "new_column_names = {'MaxHighRounded': 'MaxHigh',\n",
    "                    'MinLowRounded': 'MinLow',\n",
    "                    'AvgCloseRounded': 'AvgClose'}\n",
    "\n",
    "# Rename columns\n",
    "for old_name, new_name in new_column_names.items():\n",
    "    df_grouped = df_grouped.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "# Show the first few rows of the DataFrame to verify the column names are renamed\n",
    "df_grouped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fedb410-9dd7-4db1-8f92-09d11fe662cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Specify output path\n",
    "df_path = '/user1/pyspark_df.csv'\n",
    "\n",
    "# Export df to csv\n",
    "df.write.csv(path = df_path, mode = 'overwrite', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9290cf8c-f475-4102-968a-5bf3fc9c3f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify output path\n",
    "df_grouped_path = '/user1/pyspark_df_grouped.csv'\n",
    "\n",
    "# Export df to csv\n",
    "df_grouped.write.csv(path = df_grouped_path, mode = 'overwrite', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28aa270-2aa8-4d19-a060-62b11650adea",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "I decided to create and output as csv two datasets, one at the lowest level of granularity where one record equals a single day timestamp which will be essential for my deep learning algorithms. The second is a grouped level which is for analysis and visualisation purposes which will be completed in a seperate notebook.\n",
    "\n",
    "Irregardless of the size of a dataset in terms of number of records, big data principles and techniques can still be applied to benefit from the benefits of file storage and processing available by using Hadoop and Apache Spark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
