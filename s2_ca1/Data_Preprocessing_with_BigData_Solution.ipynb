{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f6c911-cd9e-4569-9c6b-e7d6badb01d6",
   "metadata": {},
   "source": [
    "# Data Preprocessing with BigData Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a6529-a818-4a13-a277-65f4ccb77c55",
   "metadata": {},
   "source": [
    "In the below notebook I wanted to test the findings explored in my literature review in relation to he enhanced processing capabilities of using Big Data solutions like Hadoop and Apache Spark in comparison to convential non BigData systems and methods like Python / Jupyter Notebookes.\n",
    "\n",
    "This notebook was created using BigData systems Hadoop and Apache Spark in Ubuntu to test the data storage and processing capabilities in comparison to convential processing methods Python / Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a21230e2-d9d3-4122-91a8-5394bdeb8688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return SparkContext, connecting Spark Cluster\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d103afbf-3add-426f-8801-80ab14f10a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc master - running locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31236158-a117-4efb-9454-7230ae8079b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from pyspark.sql.functions import col, mean, regexp_replace\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc58793-caa5-4e68-864c-f6b0371785a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================================================>  (21 + 1) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 33.54 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define start time variable\n",
    "start_time = time.time()\n",
    "\n",
    "# Read csv file from Hadoop directory\n",
    "df = spark.read.csv('/user1/Books_rating.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Define end time variable\n",
    "end_time = time.time()\n",
    "\n",
    "# Print result of start & end time\n",
    "print('Execution Time: {:.2f} seconds'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf471cb-4a1d-4e57-9cd2-35ffb7f90f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Price: string (nullable = true)\n",
      " |-- User_id: string (nullable = true)\n",
      " |-- profileName: string (nullable = true)\n",
      " |-- review/helpfulness: string (nullable = true)\n",
      " |-- review/score: string (nullable = true)\n",
      " |-- review/time: string (nullable = true)\n",
      " |-- review/summary: string (nullable = true)\n",
      " |-- review/text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display df structure\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8472d755-9648-409e-8a74-4fad7ecccd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 0.06 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define start time variable\n",
    "start_time = time.time()\n",
    "\n",
    "# DataType conversion\n",
    "df = df.withColumn('review/score', col('review/score').cast('float'))\n",
    "\n",
    "# Define end time variable\n",
    "end_time = time.time()\n",
    "\n",
    "# Print result of start & end time\n",
    "print('Execution Time: {:.2f} seconds'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ce0598-2838-48f8-a2e4-6136e70d47da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===================================================>     (20 + 2) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 23.93 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define start time variable\n",
    "start_time = time.time()\n",
    "\n",
    "# Handling missing values\n",
    "df = df.fillna({'Price': df.agg({'Price': 'mean'}).first()[0]})\n",
    "\n",
    "# Define end time variable\n",
    "end_time = time.time()\n",
    "\n",
    "# Print result of start & end time\n",
    "print('Execution Time: {:.2f} seconds'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42633b1-8aca-411d-9b3f-111aa954e23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define start time variable\n",
    "start_time = time.time()\n",
    "\n",
    "# Read csv file from Hadoop directory\n",
    "df = df.drop('User_id', 'profileName')\n",
    "\n",
    "# Define end time variable\n",
    "end_time = time.time()\n",
    "\n",
    "# Print result of start & end time\n",
    "print('Execution Time: {:.2f} seconds'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
