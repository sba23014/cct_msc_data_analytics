{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ce8370d",
   "metadata": {},
   "source": [
    "# Tutorial 5\n",
    "# PCA and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c62214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install this library\n",
    "!pip install PCA\n",
    "\n",
    "# If you face an error, then try on anaconda prompt\n",
    "# conda install -c bioconda bioconductor-pcatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # We can suppress the warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd67717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data set\n",
    "# Import the dataset and distributing the dataset into X and y components for data analysis.\n",
    "\n",
    "\n",
    "# importing or loading the dataset\n",
    "dataset = pd.read_csv('Wine.csv')\n",
    " \n",
    "# distributing the dataset into two components X and Y\n",
    "X = dataset.iloc[:, 0:13].values\n",
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c1aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the X and Y into the Training set and Testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data set into train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ded67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing preprocessing part\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create and initialise an object (sc) by calling a method named as StandardScaler()\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Train the model by calling a method fit_transform()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "\n",
    "# Transform the data into standised form\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA function on trainingv and testing set of X component\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create and initialise an object (pca) by calling a method PCA\n",
    "pca = PCA(n_components = 3)\n",
    "\n",
    "# Transform the data into traning and testing\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    " \n",
    "# Store the explauned variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0f63d",
   "metadata": {},
   "source": [
    "## Display PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d05718",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1,len(pca.explained_variance_ ) + 1), pca.explained_variance_ )\n",
    "plt.ylabel('Explained variance')\n",
    "plt.xlabel('Components')\n",
    "plt.plot(range(1, len(pca.explained_variance_ ) + 1), pca.explained_variance_,\n",
    "         c = 'red',\n",
    "         label = \"Cumulative Explained Variance\")\n",
    "plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5907655",
   "metadata": {},
   "source": [
    "### Cumulative variance\n",
    "Amount of variance of the original data explained by each type of model plotted against the number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaaa87e",
   "metadata": {},
   "source": [
    "# PCA for Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries for the cancer datasert\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8726e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are using inbuilt dataset of scikit learn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "  \n",
    "# instantiating\n",
    "cancer = load_breast_cancer()\n",
    "  \n",
    "# creating dataframe\n",
    "df = pd.DataFrame(cancer['data'], columns = cancer['feature_names'])\n",
    "  \n",
    "# checking head of dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc26801",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a956a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing standardscalar module \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "  \n",
    "scalar = StandardScaler()\n",
    "  \n",
    "# fitting\n",
    "scalar.fit(df)\n",
    "scaled_data = scalar.transform(df)\n",
    "  \n",
    "# Importing PCA\n",
    "from sklearn.decomposition import PCA\n",
    "  \n",
    "# Let's say, components = 2\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(scaled_data)\n",
    "x_pca = pca.transform(scaled_data)\n",
    "  \n",
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21e1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving a larger plot\n",
    "plt.figure(figsize = (8, 6))\n",
    "  \n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1], c = cancer['target'], cmap ='plasma')\n",
    "  \n",
    "# labeling x and y axes\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb552a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display components\n",
    "pca.components_, pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = pd.DataFrame(pca.components_, columns = cancer['feature_names'])\n",
    "  \n",
    "plt.figure(figsize =(14, 6))\n",
    "  \n",
    "# plotting heatmap\n",
    "sns.heatmap(df_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5afe3e",
   "metadata": {},
   "source": [
    "# K-means clustering\n",
    "\n",
    "The K-means algorithm is also referred to as vector quantization. What the algorithm does is finds the cluster (centroid) positions that minimize the distances to all points in the cluster. This is done iteratively; the problem with the algorithm is that it can be a bit greedy, meaning that it will find the nearest minima quickly. This is generally solved with some kind of basin-hopping approach where the nearest minima found is randomly perturbed and the algorithm restarted. Due to this fact, the algorithm is dependent on good initial guesses as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# generate synthetic two-dimensional data\n",
    "X, y = make_blobs(random_state = 1)\n",
    "\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the clustering model\n",
    "kmeans = KMeans(n_clusters = 3)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0159a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205412ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers = 'o')\n",
    "mglearn.discrete_scatter(\n",
    "kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2], markers = '^', markeredgewidth = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "\n",
    "# using two cluster centers:\n",
    "kmeans1 = KMeans(n_clusters =  2)\n",
    "kmeans1.fit(X)\n",
    "assignments = kmeans1.labels_\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax = axes[0], markers = 'o')\n",
    "mglearn.discrete_scatter(\n",
    "kmeans1.cluster_centers_[:, 0], kmeans1.cluster_centers_[:, 1], [0, 1], ax = axes[0], markers = '^', markeredgewidth = 4)\n",
    "\n",
    "# using five cluster centers:\n",
    "kmeans2 = KMeans(n_clusters = 5)\n",
    "kmeans2.fit(X)\n",
    "assignments = kmeans2.labels_\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax = axes[1], markers = 'o')\n",
    "mglearn.discrete_scatter(\n",
    "kmeans2.cluster_centers_[:, 0], kmeans2.cluster_centers_[:, 1], [0, 1, 2, 3, 4], ax = axes[1], markers = '^', markeredgewidth = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62804ea",
   "metadata": {},
   "source": [
    "## Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the optimum number of clusters for k-means classification\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []                  # Declare an array\n",
    "\n",
    "# Set the loop from the minimum and maximum values\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, max_iter = 300, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    print(kmeans.inertia_)\n",
    "# inertia_float: Sum of squared distances of samples to their closest cluster center.\n",
    "\n",
    "# Plotting the results onto a line graph, allowing us to observe 'The elbow'\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')    # within cluster sum of squares\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598f87f",
   "metadata": {},
   "source": [
    "## Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16234e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Instantiate the KMeans models\n",
    "km = KMeans(n_clusters = 2, random_state=42)\n",
    "\n",
    "# Fit the KMeans model\n",
    "km.fit_predict(X)\n",
    "\n",
    "# Calculate Silhoutte Score\n",
    "score = silhouette_score(X, km.labels_, metric='euclidean')\n",
    "\n",
    "# Print the score\n",
    "print('Silhouetter Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3b43ab",
   "metadata": {},
   "source": [
    "Clusters       Silhoutte Score\n",
    "2                 0.76\n",
    "3                 0.77\n",
    "4                 0.600\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7f3dc",
   "metadata": {},
   "source": [
    "Assume you own a grocery mall and have access to basic information on your customers via membership cards, such as Customer ID, age, gender, annual income, and spending score. Based on your specified criteria, such as customer behavior and purchasing information, you can assign the customer a spending score. In order to reward or promote your consumers, you as the owner would like to understand their behavior. So that your marketing team plan the strategy accordingly. The dataset is provided on Moodle.\n",
    "\n",
    "## How to use Machine Learning (kMeans clustering) algorithm to help the owner of the grocery mall using  Age and Annual Income (k$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#importing the Iris dataset with pandas\n",
    "dataset = pd.read_csv('MallCustomers.csv')\n",
    "\n",
    "# Load 4 columns of the Iris data values\n",
    "x = dataset.iloc[:, [1, 2]].values\n",
    "\n",
    "# Show first five records\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KMeans library for clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Applying KMeans to the dataset/ Creating the kmeans classifier\n",
    "kmeans = KMeans(n_clusters = 4, max_iter = 300, n_init = 10, random_state = 0)\n",
    "\n",
    "# n_initint, default = 10, Number of time the k-means algorithm will be run with different centroid seeds. \n",
    "# The final results will be the best output of n_init consecutive runs in terms of inertia.\n",
    "y_kmeans = kmeans.fit_predict(x)\n",
    "\n",
    "print(y_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the clusters\n",
    "plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 50, c = 'red', label = 'class 1')\n",
    "plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'class 2')\n",
    "plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 50, c = 'green', label = 'class 3')\n",
    "plt.scatter(x[y_kmeans == 3, 0], x[y_kmeans == 3, 1], s = 50, c = 'brown', label = 'class 4')\n",
    "# For two clusters, remove the second last two python statements from the above four python statements\n",
    "\n",
    "# Plotting the centroids of the clusters\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1:2], s = 100, c = 'yellow', label = 'Centroids')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Annual income')\n",
    "\n",
    "# A legend is an area describing the elements of the graph. In the matplotlib library, there's a function called legend() \n",
    "# which is used to Place a legend on the axes.\n",
    "plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the optimum number of clusters for k-means classification\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []                  # Declare an array\n",
    "\n",
    "# Set the loop from the minimum and maximum values\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, max_iter = 300, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(x)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "# inertia_float: Sum of squared distances of samples to their closest cluster center.\n",
    "\n",
    "# Plotting the results onto a line graph, allowing us to observe 'The elbow'\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')    # within cluster sum of squares\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d81b7",
   "metadata": {},
   "source": [
    "# Case Study - Homework\n",
    "## Suicide rate vs. GDP vs. absolute Latitude\n",
    "Chapter 5, Mastering Python Data Analysis, Magnus Vilhelm Persson, Luiz Felipe Martins, Packt Publishing, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy.random as rnd\n",
    "import scipy.stats as st\n",
    "import scipy.cluster.hierarchy as hac\n",
    "import scipy.cluster.vq as vq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc7df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_FILE = 'data.h5'\n",
    "d2 = pd.read_hdf(TABLE_FILE)\n",
    "d2 = d2.dropna()\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc329cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = d2[['DFE','GDP_CD','Both']].values.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ce0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(12, figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "plt.hist(rates.T[1], bins=20, color='SteelBlue', alpha=0.5, histtype='bar', ec='black')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('GDP')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(rates.T[0], rates.T[2], s=5e2*rates.T[1]/rates.T[1].max(),\n",
    "           color='SteelBlue', edgecolors='0.3');\n",
    "plt.xlabel('Absolute Latitude (Degrees, \\'DFE\\')')\n",
    "plt.ylabel('Suicide Rate (per 100\\')')\n",
    "plt.subplots_adjust(wspace=0.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448bbe13",
   "metadata": {},
   "source": [
    "The scatter plot to the right shows the Suicide Rate on the y-axis and the Absolute Latitude on the x-axis. The size of each point is proportional to the country's GDP. The function to run the clustering k-means takes a special kind of normalized input. The data arrays (columns) have to be normalized by the standard deviation of the array. Although this is straightforward, there is a function included in the module called whiten. It will scale the data with the standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = vq.whiten(rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188db86",
   "metadata": {},
   "source": [
    "To show what it does to the data, we plot the preceding plots again, but with the output from the whiten function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f1d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(12, figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "plt.hist(w[:,1], bins=20, color='SteelBlue', alpha=0.5, histtype='bar', ec='black')\n",
    "plt.yscale('log')\n",
    "plt.subplot(122)\n",
    "plt.scatter(w.T[0], w.T[2], s=5e2*w.T[1]/w.T[1].max(), \n",
    "            color='SteelBlue', edgecolors='0.3')\n",
    "plt.xticks(rotation=45, ha='right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0af6f",
   "metadata": {},
   "source": [
    "As you can see, all the data is scaled from the previous figure. However, as mentioned, the scaling is just the standard deviation. So let's calculate the scaling and save it to the sc variable.\n",
    "\n",
    "Now we are ready to estimate the initial guesses for the cluster centroids. Reading off the first plot of the data, we guess the centroids to be at 20 DFE, 200,000 GDP, and 10 suicides, and the second at 45 DFE, 100,000 GDP, and 15 suicides. We put this in an array and scale it with our scale parameter to the same scale as the output from the whiten function. This is then sent to the kmeans2 function of SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b132b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_guess = np.array([[20,20E3,10],[45,100E3,15]])\n",
    "sc = rates.std(axis=0)\n",
    "init_guess /= sc\n",
    "\n",
    "z2_cb, z2_lbl = vq.kmeans2(w, init_guess, minit='matrix', iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4eb368",
   "metadata": {},
   "source": [
    "There is another function, kmeans (without the 2), which is a less complex version and does not stop iterating when it reaches a local minima; it stops when the changes between two iterations goes below some level. Thus, the standard k-means algorithm is represented in SciPy by the kmeans2 function. The function outputs the centroids' scaled positions (here, z2_cb) and a lookup table (z2_lbl) telling us which row belongs to which centroid. To get the centroid positions in units we understand, we simply multiply with our scaling value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496036e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2_cb_sc = z2_cb * sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c191f",
   "metadata": {},
   "source": [
    "At this point, we can plot the results. The following section is rather long and contains many different parts, so we will go through them section by section. However, the code should be run in one cell of the Notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51621e8",
   "metadata": {},
   "source": [
    "The last tweak to the plot is made by creating a custom legend. We want to show the different sizes of the points and what GDP they correspond to. As there is a continuous gradient from low to high, we cannot use the plotted points. Thus we create our own, but leave the x and y input coordinates as empty lists. This will not show anything in the plot but we can use them to register in the legend. The various tweaks to the legend function control different aspects of the legend layout. I encourage you to experiment with it to see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37f2cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(z2_cb_sc[0,0], z2_cb_sc[0,2], \n",
    "            s=5e2*z2_cb_sc[0,1]/rates.T[1].max(), \n",
    "            marker='+', color='k', edgecolors='k', \n",
    "            lw=2, zorder=10, alpha=0.7);\n",
    "plt.scatter(z2_cb_sc[1,0], z2_cb_sc[1,2], \n",
    "            s=5e2*z2_cb_sc[1,1]/rates.T[1].max(), \n",
    "            marker='+', color='k', edgecolors='k', \n",
    "            lw=3, zorder=10, alpha=0.7);\n",
    "\n",
    "s0 = abs(z2_lbl==0).astype('bool')\n",
    "s1 = abs(z2_lbl==1).astype('bool')\n",
    "pattern1 = 5*'x'\n",
    "pattern2 = 4*'/'\n",
    "plt.scatter(w.T[0][s0]*sc[0], \n",
    "            w.T[2][s0]*sc[2], \n",
    "            s=5e2*rates.T[1][s0]/rates.T[1].max(),\n",
    "            lw=1,\n",
    "            hatch=pattern1,\n",
    "            edgecolors='0.3',\n",
    "            color=plt.cm.Blues_r(\n",
    "                rates.T[1][s0]/rates.T[1].max()));\n",
    "plt.scatter(rates.T[0][s1],\n",
    "            rates.T[2][s1], \n",
    "            s=5e2*rates.T[1][s1]/rates.T[1].max(),\n",
    "            lw=1,\n",
    "            hatch=pattern2,\n",
    "            edgecolors='0.4',\n",
    "            marker='s',\n",
    "            color=plt.cm.Reds_r(\n",
    "                rates.T[1][s1]/rates.T[1].max()+0.4))\n",
    "\n",
    "for i in range(len(rates.T[0][s0])):\n",
    "    plt.plot([z2_cb_sc[0,0], rates.T[0][s0][i]],\n",
    "             [z2_cb_sc[0,2], rates.T[2][s0][i]], \n",
    "             color='SteelBlue', lw=2, alpha=0.4, \n",
    "             zorder=-1)\n",
    "for i in range(len(rates.T[0][s1])):\n",
    "    plt.plot([z2_cb_sc[1,0], rates.T[0][s1][i]],\n",
    "             [z2_cb_sc[1,2], rates.T[2][s1][i]], \n",
    "             color='IndianRed', lw=2, alpha=0.4, \n",
    "             zorder=-1)\n",
    "\n",
    "# create some *empty* patches to use for legend, \n",
    "p1 = plt.scatter([],[], hatch='None', \n",
    "                 s=20E3*5e2/rates.T[1].max(), \n",
    "                 color='k', edgecolors='None',)\n",
    "p2 = plt.scatter([],[], hatch='None',\n",
    "                 s=40E3*5e2/rates.T[1].max(),  \n",
    "                 color='k', edgecolors='None',)\n",
    "p3 = plt.scatter([],[], hatch='None',\n",
    "                 s=60E3*5e2/rates.T[1].max(), \n",
    "                 color='k', edgecolors='None',)\n",
    "p4 = plt.scatter([],[], hatch='None',\n",
    "                 s=80E3*5e2/rates.T[1].max(), \n",
    "                 color='k', edgecolors='None',)\n",
    "\n",
    "labels = [\"20\\'\", \"40\\'\", \"60\\'\", \">80\\'\"]\n",
    "\n",
    "plt.legend([p1, p2, p3, p4], labels, ncol=1, \n",
    "           frameon=True, handlelength=1, \n",
    "           loc=1, borderpad=0.75,labelspacing=0.75,\n",
    "           handletextpad=0.75, title='GDP')\n",
    "plt.ylim((-4,40))\n",
    "plt.xlim((-4,80))\n",
    "plt.title('K-means clustering')\n",
    "plt.xlabel('Absolute Lattitude (Degrees, \\'DFE\\')')\n",
    "plt.ylabel('Suicide Rate (per 100 000)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dcaa0a",
   "metadata": {},
   "source": [
    "As for the final analysis, two different clusters are identified. Just as our previous hypothesis, there is a cluster with a clear linear trend with relatively higher GDP, which is also located at a higher absolute latitude. Although the identification is rather weak, it is clear that the two groups are separated. Countries with low GDP are clustered closer to the equator. What happens when you add more clusters? Try to add a cluster for the low DFE high-rate countries, visualize it, and think about what this could mean for the conclusion(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca35a66",
   "metadata": {},
   "source": [
    "# Task 1:\n",
    "- Apply kMeans clustering for the provided data set (weatherAUS.csv) on Moodle in the same folder. Use Loc, Latitude and Longitude columns to form clusters of Australian cities. Use Elbow Method to decide about the number of clusters.\n",
    "- Apply PCA for iris data set available on Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58990533",
   "metadata": {},
   "source": [
    "## References\n",
    "* <p>https://www.geeksforgeeks.org/ml-principal-component-analysispca</p>\n",
    "* The Complete Machine Learning Course with Python, Anthony NG and Rob Percival, Packt PublishingOctober 2018.\n",
    "* Introduction to Machine Learning with Python A Guide for Data Scientists, Andreas C. Müller and Sarah Guido, Copyright © 2017, O'Reilly.\n",
    "* Mastering Python Data Analysis, Magnus Vilhelm Persson, Luiz Felipe Martins, Packt Publishing, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
